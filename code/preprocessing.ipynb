{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#NLP libraries\n",
    "import nltk \n",
    "from nltk.corpus import udhr #corpora with texts \n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = {}\n",
    "\n",
    "# Load spacy models for languages with pipelines\n",
    "nlp[\"es\"] = spacy.load(\"es_core_news_sm\") #Spanish\n",
    "nlp[\"ko\"] = spacy.load(\"ko_core_news_sm\") #Korean\n",
    "nlp[\"fi\"] = spacy.load(\"fi_core_news_sm\") #Finnish\n",
    "nlp[\"ja\"] = spacy.load(\"ja_core_news_sm\") #Japanese\n",
    "nlp[\"pl\"] = spacy.load(\"pl_core_news_sm\") #Polish\n",
    "nlp[\"de\"] = spacy.load(\"de_core_news_sm\") #German\n",
    "\n",
    "#other spacy models for less explored languages \n",
    "from spacy.lang.tr import Turkish\n",
    "nlp[\"tr\"] = Turkish()\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp[\"id\"] = Indonesian()\n",
    "from spacy.lang.tl import Tagalog\n",
    "nlp[\"tl\"] = Tagalog()\n",
    "from spacy.lang.eu import Basque\n",
    "nlp[\"eu\"] = Basque()\n",
    "from spacy.lang.et import Estonian\n",
    "nlp[\"et\"] = Estonian()\n",
    "from spacy.lang.kn import Kannada\n",
    "nlp[\"kn\"] = Kannada()\n",
    "from spacy.lang.yo import Yoruba \n",
    "nlp[\"yo\"] = Yoruba()\n",
    "from spacy.lang.ms import Malay\n",
    "nlp[\"ms\"] = Malay()\n",
    "from spacy.lang.ga import Irish\n",
    "nlp[\"ga\"] = Irish()\n",
    "from spacy.lang.tn import Setswana\n",
    "nlp[\"tn\"] = Setswana()\n",
    "from spacy.lang.bg import Bulgarian\n",
    "nlp[\"bg\"] = Bulgarian()\n",
    "\n",
    "# from spacy.lang.ch import Chamorro\n",
    "# nlp[\"ch\"] = Chamorro()\n",
    "# from spacy.lang.kl import Greenlandic\n",
    "# nlp[\"kl\"] = Greenlandic()\n",
    "# nlp[\"que\"] = Add Quechua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Family</th>\n",
       "      <th>ISO code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korean</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tagalog</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Basque</td>\n",
       "      <td>Language isolate</td>\n",
       "      <td>eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Estonian</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kannada</td>\n",
       "      <td>Dravian</td>\n",
       "      <td>kn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yoruba</td>\n",
       "      <td>Niger-Congo</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Polish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>German</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Setswana</td>\n",
       "      <td>Niger-Congo</td>\n",
       "      <td>tn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Malay</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Irish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>ga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language            Family ISO code\n",
       "0      Spanish     Indo-European       es\n",
       "1       Korean          Koreanic       ko\n",
       "2      Finnish            Uralic       fi\n",
       "3      Turkish            Turkic       tr\n",
       "4   Indonesian      Austronesian       id\n",
       "5     Japanese           Japonic       ja\n",
       "6      Tagalog      Austronesian       tl\n",
       "7       Basque  Language isolate       eu\n",
       "8     Estonian            Uralic       et\n",
       "9      Kannada           Dravian       kn\n",
       "10      Yoruba       Niger-Congo       yo\n",
       "11      Polish     Indo-European       pl\n",
       "12      German     Indo-European       de\n",
       "13   Bulgarian     Indo-European       bg\n",
       "14    Setswana       Niger-Congo       tn\n",
       "15       Malay      Austronesian       ms\n",
       "16       Irish     Indo-European       ga"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_data= pd.read_csv(\"../data/language_data.csv\", sep=\";\")\n",
    "language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to /Users/bunetz/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Download languages\n",
    "nltk.download('udhr')\n",
    "\n",
    "all_file_ids= nltk.corpus.udhr.fileids()\n",
    "for _, row in language_data.iterrows():\n",
    "    for file_id in all_file_ids:\n",
    "        if len(re.findall(row['Language'], file_id)) > 0:\n",
    "            file = open(\"../data/\" + str(row[\"ISO code\"]) + \".txt\", \"w\")\n",
    "            file.write(nltk.corpus.udhr.raw(file_id))\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_raw_texts(list_of_codes):\n",
    "  dict_raw_texts= {}\n",
    "  for code in list_of_codes:\n",
    "    file = open(\"../data/\" + str(code) + \".txt\", \"r\")\n",
    "    dict_raw_texts[code] = file.read()\n",
    "    file.close()\n",
    "  return dict_raw_texts\n",
    "\n",
    "def tokenizer(text, model_lang):\n",
    "    doc = model_lang(text)\n",
    "    tokens = [token.text for token in doc if not token.is_space and not token.is_punct and not token.is_digit]\n",
    "    return tokens\n",
    "\n",
    "def tokens(dict_raw_texts):\n",
    "    tokens_langs= {}\n",
    "    for code in dict_raw_texts.keys():\n",
    "        tokens_langs[code] = tokenizer(dict_raw_texts[code], nlp[code])\n",
    "    return tokens_langs\n",
    "\n",
    "tokenized_languages = tokens(get_dict_raw_texts(language_data['ISO code'].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def process(tokens):\n",
    "    token_freq = Counter(tokens)\n",
    "    matrix = []\n",
    "    for token in set(tokens):\n",
    "        matrix.append([token, len(token), token_freq[token]])\n",
    "    matrix.sort(key=lambda x: x[2], reverse=True)\n",
    "    return matrix\n",
    "\n",
    "tokenized_languages_matrices= {}\n",
    "for code in tokenized_languages.keys():\n",
    "    tokenized_languages_matrices[code] = process(tokenized_languages[code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def matrix_to_csv(matrix, filename):\n",
    "    file = open(filename, 'w', newline='', encoding=\"utf-8\")\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Token', 'Length', 'Frequency'])\n",
    "    for row in matrix:\n",
    "        writer.writerow(row)\n",
    "    file.close()\n",
    "\n",
    "for code in tokenized_languages_matrices.keys():\n",
    "    filename = f\"preprocessing_data/{code.lower()}.csv\"\n",
    "    matrix_to_csv(tokenized_languages_matrices[code], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "[  1.  102.6 204.2 305.8 407.4 509. ]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6]\n",
      "[  1.  102.6 204.2 305.8 407.4 509. ]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6]\n",
      "[  1.  102.6 204.2 305.8 407.4 509. ]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6]\n",
      "[  1.  102.6 204.2 305.8 407.4 509. ]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6]\n",
      "[  1.  102.6 204.2 305.8 407.4 509. ]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import theilslopes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multiplicative_binning(r, slope, intercept):\n",
    "    smoothed_frequency = np.exp(r * slope + intercept)\n",
    "    return smoothed_frequency\n",
    "\n",
    "for lang in tokenized_languages.keys():\n",
    "    print(lang)\n",
    "\n",
    "    matrix = np.array(tokenized_languages_matrices[lang])\n",
    "    frequency = matrix[:, 2].astype(int)\n",
    "    log_frequency = np.log(frequency)\n",
    "    rank = list(range(1, len(frequency)+1))\n",
    "\n",
    "    num_bins = 5\n",
    "    bins = np.linspace(min(rank), max(rank), num_bins + 1)\n",
    "    bin_indices = np.digitize(rank, bins)\n",
    "\n",
    "    smoothed_frequency = np.zeros_like(frequency, dtype=float)\n",
    "    log_frequency_bins = []\n",
    "\n",
    "    last_index = 1\n",
    "    data\n",
    "    for i in range(1, len(bins)):\n",
    "\n",
    "        print(bins)\n",
    "        print(bin_indices)\n",
    "        # mask = bin_indices == i\n",
    "        # bin_rank = rank[mask]\n",
    "        # bin_frequency = frequency[mask]\n",
    "        # slope, intercept, _, _, _ = linregress(bin_rank, bin_frequency)\n",
    "        # smoothed_frequency[mask] = bin_rank * slope + intercept\n",
    "\n",
    "    break\n",
    "    matrix = np.array(tokenized_languages_matrices[lang])\n",
    "    frequency = matrix[:, 2].astype(int)\n",
    "    log_frequency = np.log(frequency)\n",
    "    rank = list(range(1, len(frequency)+1))\n",
    "    print(rank)\n",
    "    log_rank = np.log(rank)\n",
    "    res = theilslopes(log_frequency, log_rank, 0.90)\n",
    "\n",
    "    theoretical_frequency = log_rank * res.slope + res.intercept\n",
    "    smoothed_frequency = multiplicative_binning(log_rank, res.slope, res.intercept)\n",
    "    print(res)\n",
    "\n",
    "    plt.scatter(log_rank, log_frequency, label='Original Data')\n",
    "    plt.plot(log_rank, smoothed_frequency, color='red', label='Smoothed Data')\n",
    "    plt.plot(log_rank, theoretical_frequency, color='green', label='Theoretical Data')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Multiplicative Binning for Curve Smoothing')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
